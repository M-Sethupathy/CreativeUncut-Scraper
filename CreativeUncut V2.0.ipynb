{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github Project - Scr - CreativeUncut.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yauod_VfFqS9",
        "JDDiFTpvFv0_",
        "NDpAhM7gsjOI",
        "mTyBBGpeozN9",
        "WV6RYjeEo59K",
        "HpxmokuUjrVi",
        "GVGZTzeUuMac",
        "fZaqKQdIW0ZZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNkrrmk6i1e5"
      },
      "source": [
        "# CreativeUncut Scraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7LDgOoauHtT"
      },
      "source": [
        " Marked for To Optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyMB36GrFm68"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uASO2JWHmCEG"
      },
      "source": [
        "# Package for mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yauod_VfFqS9"
      },
      "source": [
        "## Including necessary packages & Functions\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKavQMlwB8UH"
      },
      "source": [
        "# Packages for Fetching and parsing web data\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "\n",
        "# Packages for managing files\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "# Packages for time management\n",
        "from datetime import datetime as dati\n",
        "from pytz import timezone    \n",
        "import time\n",
        "\n",
        "# Packages for multiprocessing\n",
        "from multiprocessing.dummy import Pool\n",
        "import multiprocessing\n",
        "\n",
        "# Packages for image handling\n",
        "import io\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "# True a flag to download truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Package for sorting list\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x21jTXT7i9a0"
      },
      "source": [
        "def dati_now():\r\n",
        "  return dati.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d_%H-%M-%S')\r\n",
        "\r\n",
        "def atoz_folders(Data_folder_Today):\r\n",
        "  # Creating folder if not exists\r\n",
        "  alphabetfolderlist = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\r\n",
        "  alphabetfolderlist.add('1')\r\n",
        "  alphabetfolderlist.add('The')\r\n",
        "  for i in alphabetfolderlist:\r\n",
        "    if not os.path.exists(Data_folder_Today+i):\r\n",
        "      os.makedirs(Data_folder_Today+i)\r\n",
        "  return alphabetfolderlist\r\n",
        "\r\n",
        "def get_scraper():\r\n",
        "  # Initializing scraper with session enabled and proxy disabled and max retries adapters enabled\r\n",
        "  adapter = requests.adapters.HTTPAdapter(max_retries=20)\r\n",
        "\r\n",
        "  Scraper = requests.Session()\r\n",
        "  Scraper.trust_env = False\r\n",
        "\r\n",
        "  Scraper.mount('https://', adapter)\r\n",
        "  Scraper.mount('http://', adapter)\r\n",
        "  return Scraper\r\n",
        "\r\n",
        "def check_VGame(check_specific_VG):\r\n",
        "  # Checking whether this set contains specific game\r\n",
        "  check_specific_VG = check_specific_VG.lower()\r\n",
        "  found = 0\r\n",
        "  print('\\nChecking whether game name \"{}\" is exist'.format(check_specific_VG))\r\n",
        "  for i in all_VG_urls_and_title:\r\n",
        "    if check_specific_VG in i[1].lower() or check_specific_VG in i[2].lower():\r\n",
        "      found += 1\r\n",
        "      print('Found {} :\\n\\t {}'.format(found, i))\r\n",
        "  if found == 0:\r\n",
        "    print('Not Found')\r\n",
        "\r\n",
        "def get_VG_info(Root_folder_Today):\r\n",
        "  # Checking the availability of target site\r\n",
        "  with Scraper.get(\"https://www.creativeuncut.com/game-art-galleries.html\") as scr_response:\r\n",
        "    if scr_response.status_code == 200:\r\n",
        "      scr_bs = bs(scr_response.content)\r\n",
        "      print('Finished Getting Main Page')\r\n",
        "\r\n",
        "      # Finding all videogames div tags\r\n",
        "      all_VGs_divs = scr_bs.find_all('div',attrs={'class':'ag'})\r\n",
        "\r\n",
        "      # Truncating first 2 tag because these are just texts\r\n",
        "      all_VGs_divs = all_VGs_divs[2:]\r\n",
        "\r\n",
        "      print('Number Of VideoGames : ', len(all_VGs_divs))\r\n",
        "\r\n",
        "      # Creating record for each videogames that contains following\r\n",
        "      # id    - id of that videogames\r\n",
        "      # title - title of that videogames\r\n",
        "      # url   - url of that videogames\r\n",
        "      all_VG_urls_and_title = []\r\n",
        "      for i in range(len(all_VGs_divs)):\r\n",
        "        url   = domain_name + all_VGs_divs[i].a['href']\r\n",
        "        title = all_VGs_divs[i].text\r\n",
        "        all_VG_urls_and_title.append([\"{:03d}\".format(i+1),title,url])\r\n",
        "\r\n",
        "      print('\\n\\nFirst 10 :\\n{}'.format('\\n'.join( [str(i) for i in all_VG_urls_and_title[:11]]  )))\r\n",
        "      print('\\n\\nLast  10 :\\n{}'.format('\\n'.join( [str(i) for i in all_VG_urls_and_title[-10:]] )))\r\n",
        "\r\n",
        "      # Writing VideoGames records to a text file\r\n",
        "      with open(Root_folder_Today + 'all_posts.txt', 'w', encoding='utf-8') as f1:\r\n",
        "        f1.write('\\n'.join(['\\n\\t'.join(i) for i in all_VG_urls_and_title]))\r\n",
        "      \r\n",
        "      return all_VG_urls_and_title\r\n",
        "    else:\r\n",
        "      print('Main Link is not working')\r\n",
        "\r\n",
        "\r\n",
        "def get_headers():\r\n",
        "\r\n",
        "  # List to store progress informations\r\n",
        "  current_r = [       # Current progress status\r\n",
        "                  [], # 0 - Error Downlaod Page  --- Not Used\r\n",
        "                  [], # 1 - Error Download Image\r\n",
        "                  [], # 2 - Empty VGs\r\n",
        "                  [], # 3 - All Files List\r\n",
        "                  0, # 4 - Total Image Count\r\n",
        "                  0, # 5 - Total Page Count\r\n",
        "                  0, # 6 - Finished VG Count            \r\n",
        "  ]\r\n",
        "\r\n",
        "  # Headers to download images\r\n",
        "  # This site lets users to download image only if it receives a get request with the \"Referer\" links to that image's page\r\n",
        "  # Ex : \r\n",
        "  # IMG URL               : https://www.creativeuncut.com/art/gallery-32/1bh-logo.jpg\r\n",
        "  # IMG Page URL (Referer): https://www.creativeuncut.com/gallery-32/1bh-logo.html\r\n",
        "\r\n",
        "  # Directly requesting IMG URL leads to the image's page (a html)\r\n",
        "  # Luckily with \"Referer\" targets to image's page, it is possible to get actual image(jpg, png, gif)\r\n",
        "  Headers = {\r\n",
        "              'User-agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36',\r\n",
        "              'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\r\n",
        "              'Accept-Encoding' : 'gzip,deflate,sdch',\r\n",
        "              'Referer' : 'https://www.creativeuncut.com/gallery-32/1bh-logo.html'\r\n",
        "            }\r\n",
        "  return current_r, Headers\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Function to Scrape every video game's images\r\n",
        "def VG_Scrape(VG):\r\n",
        "\r\n",
        "  # Recording time took for each Video Game\r\n",
        "  vg_scrape_time = time.time()\r\n",
        "\r\n",
        "  # Assigning folder path\r\n",
        "  if VG[1][0].upper() in alphabetfolderlist:\r\n",
        "    VG_Initial = VG[1][0].upper()\r\n",
        "    if VG[1][:4].upper() == 'THE ':\r\n",
        "      VG_Initial = 'The'\r\n",
        "  else:\r\n",
        "    VG_Initial = '1'\r\n",
        "\r\n",
        "  # Creating folder name for given Video Game\r\n",
        "  valid_VG_Title_for_windows = ''\r\n",
        "  windows_illegal_chars = {'?', ':', '/', '//','\\\\', '*', '\"', '<', '>', '|'}\r\n",
        "  for i in VG[1]:\r\n",
        "    if i in windows_illegal_chars:\r\n",
        "      valid_VG_Title_for_windows += '_'\r\n",
        "    else:  \r\n",
        "      valid_VG_Title_for_windows += i\r\n",
        "\r\n",
        "  VG_folder = f'{Data_folder_Today}{VG_Initial}/{valid_VG_Title_for_windows}/'\r\n",
        "  \r\n",
        "  # Creating folder if not exists\r\n",
        "  if not os.path.exists(VG_folder):\r\n",
        "    os.makedirs(VG_folder)\r\n",
        "\r\n",
        "  # Checking if this Video game is already downloaded or not\r\n",
        "  if not os.path.exists(VG_folder + 'Details.txt'):\r\n",
        "\r\n",
        "    # Initializing counters\r\n",
        "    img_no = 0\r\n",
        "    current_page_no = 0\r\n",
        "    img_url_list = []\r\n",
        "\r\n",
        "    # Getting Video game page URL\r\n",
        "    current_page_url = VG[2]\r\n",
        "\r\n",
        "    # Requesting page\r\n",
        "    Scr_response = Scraper.get(VG[2],headers=Headers)\r\n",
        "    \r\n",
        "    # If status code of response is 200 then it is good to go\r\n",
        "    if Scr_response.status_code != 200:\r\n",
        "\r\n",
        "      # Error requesting page, so add it to error list\r\n",
        "      current_r[0].append([VG[1],VG[2]])\r\n",
        "      # Terminating current Video game scraping\r\n",
        "      return False\r\n",
        "    else:\r\n",
        "\r\n",
        "      # Parsing the page content\r\n",
        "      Scr_bs = bs(Scr_response.content, 'html.parser')\r\n",
        "      \r\n",
        "      # Getting all page URLs\r\n",
        "      pages_list = Scr_bs.find('div',attrs={'class':'r_float'})\r\n",
        "      next_pages = pages_list.find_all('a',attrs={'class':'gn'})\r\n",
        "      no_of_pages = len(next_pages) + 1\r\n",
        "\r\n",
        "      # Traversing through each page\r\n",
        "      for _ in range(no_of_pages):\r\n",
        "\r\n",
        "        # Initializing image counter\r\n",
        "        all_img_in_current_page = []\r\n",
        "        \r\n",
        "        # Getting div tag from class \"glry\"\r\n",
        "        # There are two kinds of classes. some VG have glry, while some have gbox. IDK why?\r\n",
        "        current_page_img_table = Scr_bs.find('div',attrs={'class':'glry'})\r\n",
        "        if current_page_img_table != None:\r\n",
        "          all_img_in_current_page = current_page_img_table.find_all('div',attrs={'class':'th'})\r\n",
        "\r\n",
        "        # Getting div tag from class \"gbox\"\r\n",
        "        current_page_img_table_box = Scr_bs.find('div',attrs={'class':'gbox'})\r\n",
        "        if current_page_img_table_box != None:\r\n",
        "          all_img_in_current_page = current_page_img_table_box.find_all('div',attrs={'class':'gl'})\r\n",
        "\r\n",
        "        # If both divs are not available then add it to \"Empy VG\" list\r\n",
        "        if current_page_img_table == None and current_page_img_table_box == None:\r\n",
        "          current_r[2].append([VG[0],img_no,no_of_pages,VG[2],VG[1]])\r\n",
        "          print('Empty Posts Found')\r\n",
        "          return False\r\n",
        "\r\n",
        "        # Traversing each image\r\n",
        "        for each_img in all_img_in_current_page:          \r\n",
        "          \r\n",
        "          # Skipping image if it's src links to next-page images like \"arrow \" image that leads to next page instead of an image\r\n",
        "          if each_img.div.a.img['src'] not in  {'imgs/next-page.gif', 'imgs/next-page.jpg', 'imgs/next-page.png'}:\r\n",
        "\r\n",
        "            # Getting current IMG url\r\n",
        "            ## Marked for To Optimize\r\n",
        "            current_img_url = domain_name + each_img.div.a.img['src'].replace('_s.jpg','.jpg').replace('_s.png','.png').replace('_s.gif','.gif')\r\n",
        "\r\n",
        "            # Increamenting image counter\r\n",
        "            img_no += 1\r\n",
        "\r\n",
        "            # Getting current image's referer link\r\n",
        "            current_img_page_url = domain_name + each_img.div.a['href']\r\n",
        "\r\n",
        "            # Adding image's url to list\r\n",
        "            img_url_list.append(current_img_url)\r\n",
        "            current_r[3].append(current_img_url)\r\n",
        "            \r\n",
        "            # Changing headers of request, so to download image\r\n",
        "            Headers['Referer'] = current_img_page_url\r\n",
        "\r\n",
        "            # Requesting image\r\n",
        "            res = Scraper.get(current_img_url,headers=Headers)\r\n",
        "            \r\n",
        "            # Assigning image's name\r\n",
        "            VG_img_name = VG_folder + \"{:03d}.{}.{}\".format(img_no, 'P-{:03d}'.format(current_page_no), current_img_url[current_img_url.rfind('/')+1:])\r\n",
        "\r\n",
        "            # Check if given response is actually an image\r\n",
        "            if 'image' not in res.headers.get(\"content-type\", ''):\r\n",
        "              \r\n",
        "              # Not an image's response, so adding to error list\r\n",
        "              current_r[1].append([VG[1],VG[2],current_img_url])\r\n",
        "              print(current_img_url,'Image NOt found for ',VG_img_name,' ',VG[1])\r\n",
        "            else:\r\n",
        "\r\n",
        "              # Good image, so getting it's binary content\r\n",
        "              i = Image.open(io.BytesIO(res.content))\r\n",
        "              # Saving image to file\r\n",
        "              i.save(VG_img_name)\r\n",
        "        \r\n",
        "        # Increamenting page counter\r\n",
        "        current_page_no += 1\r\n",
        "\r\n",
        "        # Check if last page reached\r\n",
        "        if no_of_pages < current_page_no:\r\n",
        "          # Getting next page url\r\n",
        "          next_page_url = domain_name + next_pages[current_page_no - 1]['href']\r\n",
        "\r\n",
        "          # Assigning current page URL\r\n",
        "          current_page_url = next_page_url\r\n",
        "          \r\n",
        "          # Requesting next page\r\n",
        "          Scr_response = Scraper.get(next_page_url,headers=Headers)\r\n",
        "\r\n",
        "          # Check if status code is 200\r\n",
        "          if Scr_response.status_code != 200:\r\n",
        "            # Error requesting page, so add it to error list\r\n",
        "            current_r[0].append([VG[1],current_page_url])\r\n",
        "            # Terminating current Video game scraping\r\n",
        "            return False\r\n",
        "          else:\r\n",
        "            # Parsing current response\r\n",
        "            Scr_bs = bs(Scr_response.content, 'html.parser')\r\n",
        "\r\n",
        "    # Increamenting page, image counter\r\n",
        "    current_r[4] += img_no\r\n",
        "    current_r[5] += current_page_no\r\n",
        "    current_r[6] += 1\r\n",
        "\r\n",
        "    # Calculating time took for each Video Game\r\n",
        "    vg_scrape_time = time.time() - vg_scrape_time\r\n",
        "\r\n",
        "    # Creating a report for Scraping VG\r\n",
        "    detail_content_raw = [\r\n",
        "                          'Title    : {}'.format(VG[1]),\r\n",
        "                          'URL      : {}'.format(VG[2]),\r\n",
        "                          'Post ID  : {}'.format(VG[0]),\r\n",
        "                          'No IMGs  : {}'.format(img_no),\r\n",
        "                          'No Pages : {}'.format(current_page_no),\r\n",
        "                          'Time Took: {:.2f} Seconds'.format(vg_scrape_time),\r\n",
        "                          '\\n',\r\n",
        "                          'IMG URLS :\\n{}'.format('\\n'.join(img_url_list))\r\n",
        "                          \r\n",
        "                          ]\r\n",
        "    # Convert report from list to string\r\n",
        "    detail_content = '\\n'.join(detail_content_raw)\r\n",
        "\r\n",
        "    # Writing report to file\r\n",
        "    with open(VG_folder + 'Details.txt','w') as f1:\r\n",
        "      f1.write(detail_content)\r\n",
        "    \r\n",
        "    # Updating status of Scraping\r\n",
        "    status(current_r[6], VG[0], img_no, current_page_no, '{:.2f}'.format(vg_scrape_time), VG[1], VG[2])\r\n",
        "  \r\n",
        "  # If given Video game is already downloaded\r\n",
        "  else:\r\n",
        "    # Increament have count and add to list\r\n",
        "    have[0] += 1\r\n",
        "    have[1].append(VG[2])\r\n",
        "    print('Have Count = ',have[0])\r\n",
        "    \r\n",
        "    # Updating progressbar by 1\r\n",
        "    # tqdm_bar.update(1)\r\n",
        " \r\n",
        "# Function to update status of scraping\r\n",
        "def status(a, b, c, d, e, f, g):\r\n",
        "  print('{:03d} - ID:{:3s} - IMGs:{:03d} - Dirs:{:03d} - {:6s}Sec - {} - {}'.format(a, b, c, d, e, f, g))\r\n",
        "\r\n",
        "  # Updating progressbar by 1\r\n",
        "  # tqdm_bar.update(1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_report():\r\n",
        "  # Report of Scraping\r\n",
        "  detail_content_raw = [\r\n",
        "                            'Scraping took     : {:.2f} Seconds ({})'.format(Scraping_imgs_time, time.strftime('%H-%M-%S', time.gmtime(Scraping_imgs_time))),\r\n",
        "                            'Total IMGs        : {}'.format(current_r[4]),\r\n",
        "                            'Total Pages       : {}'.format(current_r[5]),\r\n",
        "                            'Total Posts       : {}'.format(current_r[6]),\r\n",
        "                            'Empty Posts {:03d}  : {}'.format(len(current_r[2]),current_r[2]),\r\n",
        "                            'Error Posts {:03d}  : {}'.format(len(current_r[0]),current_r[0]),\r\n",
        "                            'Error IMG   {:03d}  : {}'.format(len(current_r[1]),current_r[1]),\r\n",
        "                            'Have Posts  {:03d}  : {}'.format(len(have[1]),have[1]),\r\n",
        "                        ]\r\n",
        "  return '\\n'.join(detail_content_raw)\r\n",
        "\r\n",
        "\r\n",
        "# Function to return file size in human understandable format\r\n",
        "def file_size(size):\r\n",
        "  n, p, d = 0, 1024, ['Bytes', 'KB', 'MB', 'GB', 'TB']\r\n",
        "  while size > p:\r\n",
        "    size /= p\r\n",
        "    n += 1\r\n",
        "  return '{:.2f} {}'.format(size, d[n])\r\n",
        "\r\n",
        "\r\n",
        "def organise_folders():\r\n",
        "\r\n",
        "  # Function to return a directory index in dictionary format\r\n",
        "  def dirs_to_dict(i='', previous=''):\r\n",
        "    \r\n",
        "    # Creating current directory path\r\n",
        "    fp = previous + i + '/'\r\n",
        "    \r\n",
        "    # Creating a sorted list of \"Directories\" in current directories\r\n",
        "    sorted_dir = sorted([k for k in next(os.walk(fp))[1]])\r\n",
        "    \r\n",
        "    # Assigning current directory name and path\r\n",
        "    temp = {\r\n",
        "              keys_list[0] : i,   # Current Directory Name\r\n",
        "              keys_list[1] : fp,  # Current Directory Path\r\n",
        "            }\r\n",
        "    \r\n",
        "    # Getting a list of files with full path in current directory\r\n",
        "    temp[keys_list[7]] =  [fp+j for j in sorted([k for k in next(os.walk(fp))[2]]) ]  \r\n",
        "\r\n",
        "    # Counting number of files in current directory\r\n",
        "    temp[keys_list[2]] = len(temp[keys_list[7]])\r\n",
        "    \r\n",
        "    # Calculating total size of files in current directory\r\n",
        "    temp[keys_list[3]] = sum([os.path.getsize(j) for j in temp[keys_list[7]]])\r\n",
        "\r\n",
        "    # Creating an empty list to store subdirectory's data\r\n",
        "    temp[keys_list[8]] = []\r\n",
        "    \r\n",
        "    # Copying number files in current directory to initialize the calculation of total number of files inside current directory (including files in subdirectory)\r\n",
        "    temp[keys_list[4]] = temp[keys_list[2]]\r\n",
        "    \r\n",
        "    # Inititalizing total directorys count to zero\r\n",
        "    temp[keys_list[5]] = 0\r\n",
        "    \r\n",
        "    # Copying size of files in current directory to initialize the calculation of total size of files inside current directory (including files in subdirectory)\r\n",
        "    temp[keys_list[6]] = temp[keys_list[3]]\r\n",
        "    \r\n",
        "    # Check if leaf directory is reached or not\r\n",
        "    # If FALSE skip this part\r\n",
        "    # If TRUE go inside \r\n",
        "    if sorted_dir != []:\r\n",
        "      \r\n",
        "      # Traversing through each directory\r\n",
        "      for j in sorted_dir:\r\n",
        "      \r\n",
        "        # Calling recursive function to it's subdirectorys and gets it's directory index\r\n",
        "        temp_dict = dirs_to_dict(j, fp)\r\n",
        "        \r\n",
        "        # Getting files, directorys, file's size\r\n",
        "        temp_Files_count, temp_Folders_count, temp_Size = temp_dict[keys_list[4]], temp_dict[keys_list[5]], temp_dict[keys_list[6]]\r\n",
        "        \r\n",
        "        # Appending subdirectory index to current directory's directory section\r\n",
        "        temp[keys_list[8]].append(temp_dict)\r\n",
        "        \r\n",
        "        # Increamenting total files count inside current directory\r\n",
        "        temp[keys_list[4]]  += temp_Files_count\r\n",
        "        \r\n",
        "        # Increamenting total directorys count inside current directory\r\n",
        "        temp[keys_list[5]]  += temp_Folders_count + 1\r\n",
        "        \r\n",
        "        # Increamenting total file's size inside current directory\r\n",
        "        temp[keys_list[6]]  += temp_Size\r\n",
        "    if '[' not in fp:\r\n",
        "      rename_list.append([fp, '{} [ {} - {} ]'.format(fp[:-1], temp[keys_list[4]], file_size(temp[keys_list[6]]))] )\r\n",
        "\r\n",
        "    # Adding a key that stores file size in readable format\r\n",
        "    temp[keys_list[9]]  = file_size(temp[keys_list[6]])\r\n",
        "\r\n",
        "    # Returning index dictionary of current directory\r\n",
        "    return temp\r\n",
        "\r\n",
        "  # Function to rename folders\r\n",
        "  def rename_folders(rename_list):\r\n",
        "    # If set 1 Rename folders\r\n",
        "    # If set 0 Undo Rename Folders\r\n",
        "    option = 1\r\n",
        "    j, k = 1-option, 1*option\r\n",
        "    for i in rename_list:\r\n",
        "      os.rename(i[j], i[k])\r\n",
        "    print('Folders renamed successfully')\r\n",
        "    \r\n",
        "\r\n",
        "  # Assigning keys for dictionary\r\n",
        "  use_short_keys = 0\r\n",
        "\r\n",
        "  # IF TRUE use shorter keys, else use longer meaningful keys\r\n",
        "  if use_short_keys == 1:\r\n",
        "    keys_list = 'FoN#P#_CFi#_CS #_TFi#_TFo#_TS #__Fi #__Fo #_TSR#4'.split('#')\r\n",
        "  else:\r\n",
        "    keys_list = 'Folder_Name   #Path          #_Current_Files#_Current_Size #_Total_Files  #_Total_Folders#_Total_Size   #__Files       #__Folders     #_TS_Readable  #14'.split('#')\r\n",
        "\r\n",
        "\r\n",
        "  # List to rename folders\r\n",
        "  # 1st value in each item points to \"Original Folder Name\"\r\n",
        "  # 2nd value in each item points to \"Renamed  Folder Name\"\r\n",
        "  rename_list = []\r\n",
        "\r\n",
        "  # Directory name to index\r\n",
        "  dir_name = Root_folder_Today[:-1]\r\n",
        "\r\n",
        "  # Calling indexing function and get the directory index\r\n",
        "  dir_data = dirs_to_dict(dir_name, '')\r\n",
        "\r\n",
        "  print('JSON data Created')\r\n",
        "\r\n",
        "  # Exclude from rename list\r\n",
        "  excluded_rename_list = [Root_folder_Today, Root_folder_Today[:-1]+'/.ipynb_checkpoints/']\r\n",
        "  rename_list = [i for i in rename_list if i[0] not in excluded_rename_list]\r\n",
        "\r\n",
        "  rename_folders(rename_list)\r\n",
        "\r\n",
        "  # Calling indexing function and get the directory index\r\n",
        "  dir_data = dirs_to_dict(dir_name, '')\r\n",
        "\r\n",
        "  # Showing information of current directory from dictionary\r\n",
        "  for i in dir_data:\r\n",
        "    if i not in [keys_list[7], keys_list[8]]:\r\n",
        "      temp = '{:' + keys_list[10] + 's} -> {}'\r\n",
        "      if i != keys_list[6]:\r\n",
        "        print(temp.format(i, dir_data[i]))\r\n",
        "      else:\r\n",
        "        print(temp.format(i, file_size(dir_data[i])))\r\n",
        "\r\n",
        "  # Assigning JSON filename\r\n",
        "  json_file_name = '{}/{} - Index.json'.format(dir_name, dir_name)\r\n",
        "\r\n",
        "  # Writing dictionary in JSON format with \"Sorted Keys\" and \"UNICODE Support\" and \"Indentation\"\r\n",
        "  with open(json_file_name, 'w') as f1:\r\n",
        "    json.dump(dir_data, f1, sort_keys=True, ensure_ascii=False, indent=4)\r\n",
        "\r\n",
        "  print('Directory Index created successfully')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDDiFTpvFv0_"
      },
      "source": [
        "## Initiating Necessary variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyFLMckoYmr8"
      },
      "source": [
        "# Initiating program execution counter\n",
        "execution_time_full = time.time()\n",
        "\n",
        "# Assigning folder names\n",
        "Today_now = dati_now()\n",
        "Root_folder_Today = f'Docs{Today_now}/'\n",
        "Data_folder_Today = Root_folder_Today + 'Data/'\n",
        "\n",
        "# Domain of target site\n",
        "domain_name = 'https://www.creativeuncut.com/'\n",
        "\n",
        "alphabetfolderlist = atoz_folders(Data_folder_Today)\n",
        "\n",
        "Scraper = get_scraper()\n",
        "\n",
        "# Creating atoz in ascii value\n",
        "ascii = [i for i in string.ascii_letters]\n",
        "\n",
        "# Assign number of threads to run\n",
        "no_of_threads = multiprocessing.cpu_count() * 2\n",
        "\n",
        "current_r, Headers = get_headers()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3h_uycwy2eq"
      },
      "source": [
        "all_VG_urls_and_title = get_VG_info(Root_folder_Today)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-1_J4vbFaxE"
      },
      "source": [
        "check_VGame('SINoALICE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORI5Mu3dta5u"
      },
      "source": [
        "## Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc2r5llHBVz7"
      },
      "source": [
        "# Creating number of threads \n",
        "pool1 = Pool(no_of_threads)\n",
        "\n",
        "# Creating progressbar wiht \"tqdm(full form 'taqaddum' means 'progress' in Arabic)\"\n",
        "# tqdm_bar = tqdm(total=len(all_VG_urls_and_title))\n",
        "\n",
        "\n",
        "# Initializing Already downloaded list\n",
        "have = [\n",
        "           0, # Already have count\n",
        "          [], # Already have post\n",
        "        ]\n",
        "\n",
        "# Recording Scraping time\n",
        "Scraping_imgs_time = time.time()\n",
        "\n",
        "# Running created threads\n",
        "_ = pool1.map(VG_Scrape,all_VG_urls_and_title)\n",
        "# Closing threads\n",
        "pool1.close()\n",
        "pool1.join()\n",
        "\n",
        "# Closing progress bar\n",
        "# tqdm_bar.close()\n",
        "\n",
        "# Calculating Scraping time\n",
        "Scraping_imgs_time = time.time() - Scraping_imgs_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huy6zqoKBQgV"
      },
      "source": [
        "detail_content = get_report()\n",
        "print(detail_content)\n",
        "\n",
        "# Writing Report to file\n",
        "with open(Data_folder_Today+'Project-Details.txt','w') as f1:\n",
        "  f1.write(detail_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDpAhM7gsjOI"
      },
      "source": [
        "## Renaming Folders\n",
        "\n",
        "  Ex:\n",
        "\n",
        "    Original Folder name : God of War\n",
        "\n",
        "    Renamed  Folder name : God of War [ 35 - 3.67 MB]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZi_LmD-ouk7"
      },
      "source": [
        "organise_folders()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTyBBGpeozN9"
      },
      "source": [
        "## Archiving Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b8YvEgoDOBf"
      },
      "source": [
        "# Calculating full program execution time\n",
        "execution_time_full = time.time() - execution_time_full\n",
        "print('Full program took :\\n{:.2f} Seconds\\n{}'.format(execution_time_full, time.strftime('%HH-%MM-%SS', time.gmtime(execution_time_full))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGptR_FJsrBn"
      },
      "source": [
        "# Archiving useing 7ZIP with no compression\n",
        "output_archieve_name = '{} - CreativeUncut ({}) ({}) ({}).7z'.format(dati_now(), current_r[6], current_r[4], time.strftime('%HH-%MM-%SS', time.gmtime(execution_time_full)))\n",
        "!7z a -m0=Copy \"$output_archieve_name\" \"$Root_folder_Today\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6RYjeEo59K"
      },
      "source": [
        "## Copying Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_3nkE4aWYX3"
      },
      "source": [
        "!rsync -ah --progress \"$output_archieve_name\" \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpxmokuUjrVi"
      },
      "source": [
        "## Deleting Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiPNxPLnjq3o"
      },
      "source": [
        "a = input('Gonna delete some (Y/N): ')\r\n",
        "if 'y' == a.lower():\r\n",
        "  print('Deleting : ', Root_folder_Today)\r\n",
        "  !rm -rf \"$Root_folder_Today\"\r\n",
        "  print('Deleted : ', Root_folder_Today)\r\n",
        "  \r\n",
        "  print('Deleting : ', output_archieve_name)\r\n",
        "  !rm -rf \"$output_archieve_name\"\r\n",
        "  print('Deleted : ', output_archieve_name)\r\n",
        "else:\r\n",
        "  print('It\"s ok you decided not TO')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVGZTzeUuMac"
      },
      "source": [
        "# Other Test Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZaqKQdIW0ZZ"
      },
      "source": [
        "##Program to Download single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNGK5CFkViql"
      },
      "source": [
        "#####################Program to Download Single image from creativeuncut.com#########################\n",
        "\n",
        "# import requests as rq\n",
        "# import io\n",
        "# from PIL import Image\n",
        "\n",
        "# Headers = {\n",
        "#             'User-agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36',\n",
        "#             'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "#             'Accept-Encoding' : 'gzip,deflate,sdch',\n",
        "#             'Referer' : 'https://www.creativeuncut.com/gallery-39/got-heavy-mongol-rank-4.html'\n",
        "#           }\n",
        "# llink = 'https://www.creativeuncut.com/gallery-39/art/got-heavy-mongol-rank-4.jpg'\n",
        "\n",
        "# rres = rq.get(llink, headers=Headers)\n",
        "# i = Image.open(io.BytesIO(rres.content))\n",
        "# i.save('1.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0QExYiFObR6"
      },
      "source": [
        "Sort by file sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu6sj6lC742u"
      },
      "source": [
        "##########################List all files with respective file size#########################\n",
        "\n",
        "# all_files_with_size = []\n",
        "# total_size = 0\n",
        "# for path, dirs, files in os.walk(start_path):\n",
        "#   for f in files:\n",
        "#     fp = os.path.join(path, f)\n",
        "#     total_size += os.path.getsize(fp)\n",
        "#     all_files_with_size.append([os.path.getsize(fp),f,fp])\n",
        "# all_files_with_size = sorted(all_files_with_size, key=itemgetter(0), reverse=True)\n",
        "\n",
        "# n = 10\n",
        "# print('First {} :\\n{}'.format(n, '\\n'.join(['{:10s}{}'.format(file_size(i[0]), i[1]) for i in all_files_with_size[:n+1]])))\n",
        "# print('Last {} :\\n{}'.format(n, '\\n'.join(['{:10s}{}'.format(file_size(i[0]), i[1]) for i in all_files_with_size[-n:]])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHN1lIL2nid1"
      },
      "source": [
        "########################Function to get total files and total size of given folder ##########################\n",
        "\n",
        "# def get_size(start_path = '.'):\n",
        "#   total_size = 0\n",
        "#   total_count = 0\n",
        "#   for path, dir, files in os.walk(start_path):\n",
        "#     for f in files:\n",
        "#       total_count += 1\n",
        "#       total_size += os.path.getsize(os.path.join(path, f))\n",
        "#   print('Total Files Size  = {}\\nTotal Files Count = {}'.format(file_size(total_size), total_count))\n",
        "\n",
        "# get_size(Data_folder_Today)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TSZyQPMOhJ3"
      },
      "source": [
        "Copy Speed comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yCToXMdFPa4"
      },
      "source": [
        "############################Speed comparison of !cp vs !rsync##################################\n",
        "\n",
        "\n",
        "# !rm \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n",
        "\n",
        "# test2 = time.time()\n",
        "# !cp \"/content/drive/My Drive/My Files/Project_Data/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\" \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n",
        "# test2 = time.time() - test2\n",
        "\n",
        "# !rm \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n",
        "\n",
        "# test1 = time.time()\n",
        "# !rsync -ah --progress \"/content/drive/My Drive/My Files/Project_Data/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\" \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n",
        "# test1 = time.time() - test1\n",
        "\n",
        "# print('rsync took {:.2f} Sec\\ncp    took {:.2f} Sec'.format(test1, test2))\n",
        "\n",
        "\n",
        "##########################Test Output###################################\n",
        "# sending incremental file list\n",
        "# 2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\n",
        "#           5.15G 100%   57.60MB/s    0:01:25 (xfr#1, to-chk=0/1)\n",
        "# rsync took 85.52 Sec\n",
        "# cp    took 77.65 Sec\n",
        "\n",
        "\n",
        "###########################Test result:################################\n",
        "#  !cp is faster than !rsync with time gap of approximately 5 seconds\n",
        "#  it is not much gap and !rsync shows progress\n",
        "#\n",
        "#  Between 5sec faster vs visual progress I will go for Visual progress so i am using rsync from now on\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CUij6HZF7fZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfxunAO1U6l8"
      },
      "source": [
        "# from IPython.display import display\r\n",
        "# import ipywidgets as widgets\r\n",
        "# import time\r\n",
        "\r\n",
        "# out = widgets.HTML()\r\n",
        "# display(out)\r\n",
        "\r\n",
        "# for i in range(50):\r\n",
        "#   time.sleep(1)\r\n",
        "#   out.value='''\r\n",
        "#   <html>\r\n",
        "#   <style>\r\n",
        "#   table.blueTable {\r\n",
        "#     border: 1px solid #1C6EA4;\r\n",
        "#     background-color: #EEEEEE;\r\n",
        "#     width: 100%;\r\n",
        "#     text-align: left;\r\n",
        "#     border-collapse: collapse;\r\n",
        "#   }\r\n",
        "#   table.blueTable td, table.blueTable th {\r\n",
        "#     border: 1px solid #AAAAAA;\r\n",
        "#     padding: 3px 2px;\r\n",
        "#   }\r\n",
        "#   table.blueTable tbody td {\r\n",
        "#     font-size: 13px;\r\n",
        "#   }\r\n",
        "#   table.blueTable tr:nth-child(even) {\r\n",
        "#     background: #D0E4F5;\r\n",
        "#   }\r\n",
        "#   table.blueTable thead {\r\n",
        "#     background: #1C6EA4;\r\n",
        "#     background: -moz-linear-gradient(top, #5592bb 0%, #327cad 66%, #1C6EA4 100%);\r\n",
        "#     background: -webkit-linear-gradient(top, #5592bb 0%, #327cad 66%, #1C6EA4 100%);\r\n",
        "#     background: linear-gradient(to bottom, #5592bb 0%, #327cad 66%, #1C6EA4 100%);\r\n",
        "#     border-bottom: 2px solid #444444;\r\n",
        "#   }\r\n",
        "#   table.blueTable thead th {\r\n",
        "#     font-size: 15px;\r\n",
        "#     font-weight: bold;\r\n",
        "#     color: #FFFFFF;\r\n",
        "#     border-left: 2px solid #D0E4F5;\r\n",
        "#   }\r\n",
        "#   table.blueTable thead th:first-child {\r\n",
        "#     border-left: none;\r\n",
        "#   }\r\n",
        "\r\n",
        "#   table.blueTable tfoot {\r\n",
        "#     font-size: 14px;\r\n",
        "#     font-weight: bold;\r\n",
        "#     color: #FFFFFF;\r\n",
        "#     background: #D0E4F5;\r\n",
        "#     background: -moz-linear-gradient(top, #dcebf7 0%, #d4e6f6 66%, #D0E4F5 100%);\r\n",
        "#     background: -webkit-linear-gradient(top, #dcebf7 0%, #d4e6f6 66%, #D0E4F5 100%);\r\n",
        "#     background: linear-gradient(to bottom, #dcebf7 0%, #d4e6f6 66%, #D0E4F5 100%);\r\n",
        "#     border-top: 2px solid #444444;\r\n",
        "#   }\r\n",
        "#   table.blueTable tfoot td {\r\n",
        "#     font-size: 14px;\r\n",
        "#   }\r\n",
        "#   table.blueTable tfoot .links {\r\n",
        "#     text-align: right;\r\n",
        "#   }\r\n",
        "#   table.blueTable tfoot .links a{\r\n",
        "#     display: inline-block;\r\n",
        "#     background: #1C6EA4;\r\n",
        "#     color: #FFFFFF;\r\n",
        "#     padding: 2px 8px;\r\n",
        "#     border-radius: 5px;\r\n",
        "#   }\r\n",
        "#   </style>\r\n",
        "\r\n",
        "#   <body>\r\n",
        "#   <table class=\"blueTable\">\r\n",
        "#   <thead>\r\n",
        "#   <tr>\r\n",
        "#   <th>head''' + str(i) + '''</th>\r\n",
        "#   <th>head2</th>\r\n",
        "#   <th>head3</th>\r\n",
        "#   <th>head4</th>\r\n",
        "#   <th>head5</th>\r\n",
        "#   <th>head6</th>\r\n",
        "#   <th>head7</th>\r\n",
        "#   </tr>\r\n",
        "#   </thead>\r\n",
        "#   <tfoot>\r\n",
        "#   <tr>\r\n",
        "#   <td colspan=\"7\">\r\n",
        "#   <div class=\"links\"><a href=\"#\">&laquo;</a> <a class=\"active\" href=\"#\">1</a> <a href=\"#\">2</a> <a href=\"#\">3</a> <a href=\"#\">4</a> <a href=\"#\">&raquo;</a></div>\r\n",
        "#   </td>\r\n",
        "#   </tr>\r\n",
        "#   </tfoot>\r\n",
        "#   <tbody>\r\n",
        "#   <tr>\r\n",
        "#   <td>cell1_1</td>\r\n",
        "#   <td>cell2_1</td>\r\n",
        "#   <td>cell3_1</td>\r\n",
        "#   <td>cell4_1</td>\r\n",
        "#   <td>cell5_1</td>\r\n",
        "#   <td>cell6_1</td>\r\n",
        "#   <td>cell7_1</td>\r\n",
        "#   </tr>\r\n",
        "#   <tr>\r\n",
        "#   <td>cell1_2</td>\r\n",
        "#   <td>cell2_2</td>\r\n",
        "#   <td>cell3_2</td>\r\n",
        "#   <td>cell4_2</td>\r\n",
        "#   <td>cell5_2</td>\r\n",
        "#   <td>cell6_2</td>\r\n",
        "#   <td>cell7_2</td>\r\n",
        "#   </tr>\r\n",
        "#   <tr>\r\n",
        "#   <td>cell1_3</td>\r\n",
        "#   <td>cell2_3</td>\r\n",
        "#   <td>cell3_3</td>\r\n",
        "#   <td>cell4_3</td>\r\n",
        "#   <td>cell5_3</td>\r\n",
        "#   <td>cell6_3</td>\r\n",
        "#   <td>cell7_3</td>\r\n",
        "#   </tr>\r\n",
        "#   <tr>\r\n",
        "#   <td>cell1_4</td>\r\n",
        "#   <td>cell2_4</td>\r\n",
        "#   <td>cell3_4</td>\r\n",
        "#   <td>cell4_4</td>\r\n",
        "#   <td>cell5_4</td>\r\n",
        "#   <td>cell6_4</td>\r\n",
        "#   <td>cell7_4</td>\r\n",
        "#   </tr>\r\n",
        "#   </tbody>\r\n",
        "#   </table>\r\n",
        "#   </body>\r\n",
        "#   </html>\r\n",
        "#   '''\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}