{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Github Project - Scr - CreativeUncut.ipynb","provenance":[{"file_id":"1elFnV2xBX0XZB1d6c9L6U4jFD7kY4l49","timestamp":1600166324461}],"collapsed_sections":[],"authorship_tag":"ABX9TyNyERX3UaFrzfW5F/NvhEMp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XERfPdSqt5_G","colab_type":"text"},"source":["# Initializing Scraping"]},{"cell_type":"markdown","metadata":{"id":"gyMB36GrFm68","colab_type":"text"},"source":["##Mounting Google Drive\n","\n"]},{"cell_type":"code","metadata":{"id":"uASO2JWHmCEG","colab_type":"code","colab":{}},"source":["# Package for mounting google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yauod_VfFqS9","colab_type":"text"},"source":["##Including necessary packages"]},{"cell_type":"code","metadata":{"id":"kKavQMlwB8UH","colab_type":"code","colab":{}},"source":["# Packages for Fetching and parsing web data\n","from bs4 import BeautifulSoup as bs\n","import requests\n","\n","# Packages for managing files\n","import os\n","import shutil\n","import json\n","\n","# Packages for time management\n","from datetime import datetime as dati\n","from pytz import timezone    \n","import time\n","\n","# Packages for multiprocessing\n","from multiprocessing.dummy import Pool\n","from tqdm.notebook import tqdm\n","\n","# Packages for image handling\n","import io\n","from PIL import Image\n","from PIL import ImageFile\n","\n","# Package for sorting list\n","from operator import itemgetter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDDiFTpvFv0_","colab_type":"text"},"source":["##Initiating Necessary variables"]},{"cell_type":"code","metadata":{"id":"RyFLMckoYmr8","colab_type":"code","colab":{}},"source":["# Initiating program execution counter\n","execution_time_full = time.time()\n","\n","# Assigning folder names\n","Today_now = dati.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d_%H-%M-%S')\n","Root_folder_Today = \"Docs \" + Today_now + '/'\n","Data_folder_Today = Root_folder_Today + 'Data/'\n","\n","# Creating folder if not exists\n","alphabetfolderlist = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n","alphabetfolderlist = [i for i in alphabetfolderlist]\n","alphabetfolderlist.extend(['1', 'The'])\n","for i in alphabetfolderlist:\n","  if not os.path.exists(Data_folder_Today+i):\n","    os.makedirs(Data_folder_Today+i)\n","\n","# Initializing scraper with session enabled and proxy disabled and max retries adapters enabled\n","MAX_RETRIES = 20\n","adapter = requests.adapters.HTTPAdapter(max_retries=MAX_RETRIES)\n","\n","Scraper = requests.Session()\n","Scraper.trust_env = False\n","\n","Scraper.mount('https://', adapter)\n","Scraper.mount('http://', adapter)\n","\n","\n","# Domain of target site\n","domain_name = 'https://www.creativeuncut.com/'\n","\n","# Creating atoz in ascii value\n","ascii = [chr(i) for i in range(97, 123)]\n","ascii.extend([chr(i) for i in range(65, 91)])\n","\n","# True a flag to download truncated images\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","# Assign number of threads to run\n","no_of_cores = 2\n","thread_multiplier = 2\n","no_of_threads = no_of_cores * thread_multiplier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3h_uycwy2eq","colab_type":"code","colab":{}},"source":["# Checking the availability of target site\n","with Scraper.get(\"https://www.creativeuncut.com/game-art-galleries.html\") as scr_response:\n","  if scr_response.status_code == 200:\n","    scr_bs = bs(scr_response.content)\n","    print('Finished Getting Main Page')\n","  else:\n","    print('Main Link is not working')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjj4rw0aDSFN","colab_type":"code","colab":{}},"source":["# Finding all videogames div tags\n","all_VGs_divs = scr_bs.find_all('div',attrs={'class':'ag'})\n","\n","# Truncating first 2 tag because these are just texts\n","all_VGs_divs = all_VGs_divs[2:]\n","\n","# Number of videogames available\n","no_of_VGs = len(all_VGs_divs)\n","\n","print('Number Of VideoGames : ',no_of_VGs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKRrsHs7EMjt","colab_type":"code","colab":{}},"source":["# Creating record for each videogames that contains following\n","# id    - id of that videogames\n","# title - title of that videogames\n","# url   - url of that videogames\n","\n","all_VG_urls_and_title = []\n","\n","for i in range(no_of_VGs):\n","  url = domain_name + all_VGs_divs[i].a['href']\n","  title = all_VGs_divs[i].text\n","  all_VG_urls_and_title.append([\"{:03d}\".format(i+1),title,url])\n","\n","print('Number of VideoGames records = {}\\n\\nFirst 10 : \\n{}\\n\\nLast 10 :\\n{}'.format(len(all_VG_urls_and_title), '\\n'.join([str(i) for i in all_VG_urls_and_title[:11]]), '\\n'.join([str(i) for i in all_VG_urls_and_title[-10:]])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-1_J4vbFaxE","colab_type":"code","colab":{}},"source":["# Checking whether this set contains specific game\n","check_specific_VG = 'SINoALICE'.lower()\n","for i in all_VG_urls_and_title:\n","  if check_specific_VG in i[1].lower() or check_specific_VG in i[2].lower():\n","    print('Found in ',i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VucDb_R5CWQU","colab_type":"code","colab":{}},"source":["# Writing VideoGames records to a text file\n","with open(Root_folder_Today + 'all_posts.txt', 'w', encoding='utf-8') as f1:\n","  f1.write('\\n'.join(['\\n\\t'.join(i) for i in all_VG_urls_and_title]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCbJoyTJVQg6","colab_type":"code","colab":{}},"source":["\n","# List to store progress informations\n","current_r = [       # Current progress status\n","                [], # 0 - Error Downlaod Page  --- Not Used\n","                [], # 1 - Error Download Image\n","                [], # 2 - Empty VGs\n","                [], # 3 - All Files List\n","                 0, # 4 - Total Image Count\n","                 0, # 5 - Total Page Count\n","                 0, # 6 - Finished VG Count            \n","]\n","\n","# Headers to download images\n","# This site lets users to download image only if it receives a get request with the \"Referer\" links to that image's page\n","# Ex : \n","# IMG URL               : https://www.creativeuncut.com/art/gallery-32/1bh-logo.jpg\n","# IMG Page URL (Referer): https://www.creativeuncut.com/gallery-32/1bh-logo.html\n","\n","# Directly requesting IMG URL leads to the image's page (a html)\n","# Luckily with \"Referer\" targets to image's page, it is possible to get actual image(jpg, png, gif)\n","Headers = {\n","            'User-agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36',\n","            'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n","            'Accept-Encoding' : 'gzip,deflate,sdch',\n","            'Referer' : 'https://www.creativeuncut.com/gallery-32/1bh-logo.html'\n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORI5Mu3dta5u","colab_type":"text"},"source":["#Scraping"]},{"cell_type":"code","metadata":{"id":"D4zSCaJvB4VW","colab_type":"code","colab":{}},"source":["# Function to Scrape every video game's images\n","def VG_Scrape(VG):\n","\n","  # Recording time took for each Video Game\n","  vg_scrape_time = time.time()\n","\n","  # Assigning folder path\n","  if VG[1][0].upper() in alphabetfolderlist:\n","    VG_Initial = VG[1][0].upper()\n","    if VG[1][:4].upper() == 'THE ':\n","      VG_Initial = 'The'\n","  else:\n","    VG_Initial = '1'\n","\n","  # Creating folder name for given Video Game\n","  valid_VG_Title_for_windows = VG[1]\n","  for i in ['?', ':', '/', '//','\\\\', '*', '\"', '<', '>', '|']:\n","    valid_VG_Title_for_windows = valid_VG_Title_for_windows.replace(i, '_') \n","\n","  VG_folder = Data_folder_Today + VG_Initial + '/' + valid_VG_Title_for_windows + '/'\n","  \n","  # Creating folder if not exists\n","  if not os.path.exists(VG_folder):\n","    os.makedirs(VG_folder)\n","\n","  # Checking if this Video game is already downloaded or not\n","  if not os.path.exists(VG_folder + 'Details.txt'):\n","\n","    # Initializing counters\n","    img_no = 0\n","    current_page_no = 0\n","    img_url_list = []\n","\n","    # Getting Video game page URL\n","    current_page_url = VG[2]\n","\n","    # Requesting page\n","    Scr_response = Scraper.get(VG[2],headers=Headers)\n","    \n","    # If status code of response is 200 then it is good to go\n","    if Scr_response.status_code != 200:\n","\n","      # Error requesting page, so add it to error list\n","      current_r[0].append([VG[1],VG[2]])\n","      # Terminating current Video game scraping\n","      return False\n","    else:\n","\n","      # Parsing the page content\n","      Scr_bs = bs(Scr_response.content, 'html.parser')\n","      \n","      # Getting all page URLs\n","      pages_list = Scr_bs.find('div',attrs={'class':'r_float'})\n","      next_pages = pages_list.find_all('a',attrs={'class':'gn'})\n","      no_of_pages = len(next_pages) + 1\n","\n","      # Traversing through each page\n","      for _ in range(no_of_pages):\n","\n","        # Initializing image counter\n","        all_img_in_current_page = []\n","        \n","        # Getting div tag from class \"glry\"\n","        # There are two kinds of classes. some VG have glry, while some have gbox. IDK why?\n","        current_page_img_table = Scr_bs.find('div',attrs={'class':'glry'})\n","        if current_page_img_table != None:\n","          all_img_in_current_page = current_page_img_table.find_all('div',attrs={'class':'th'})\n","\n","        # Getting div tag from class \"gbox\"\n","        current_page_img_table_box = Scr_bs.find('div',attrs={'class':'gbox'})\n","        if current_page_img_table_box != None:\n","          all_img_in_current_page = current_page_img_table_box.find_all('div',attrs={'class':'gl'})\n","\n","        # If both divs are not available then add it to \"Empy VG\" list\n","        if current_page_img_table == None and current_page_img_table_box == None:\n","          current_r[2].append([VG[0],img_no,no_of_pages,VG[2],VG[1]])\n","          print('Empty Posts Found')\n","          return False\n","\n","        # Traversing each image\n","        for each_img in all_img_in_current_page:          \n","          \n","          # Skipping image if it's src links to next-page images like \"arrow \" image that leads to next page instead of an image\n","          if each_img.div.a.img['src'] not in  ['imgs/next-page.gif', 'imgs/next-page.jpg', 'imgs/next-page.png']:\n","\n","            # Getting current IMG url\n","            current_img_url = domain_name + each_img.div.a.img['src'].replace('_s.jpg','.jpg').replace('_s.png','.png').replace('_s.gif','.gif')\n","\n","            # Increamenting image counter\n","            img_no += 1\n","\n","            # Getting current image's referer link\n","            current_img_page_url = domain_name + each_img.div.a['href']\n","\n","            # Adding image's url to list\n","            img_url_list.append(current_img_url)\n","            current_r[3].append(current_img_url)\n","            \n","            # Changing headers of request, so to download image\n","            Headers['Referer'] = current_img_page_url\n","\n","            # Requesting image\n","            res = Scraper.get(current_img_url,headers=Headers)\n","            \n","            # Assigning image's name\n","            VG_img_name = VG_folder + \"{:03d}.{}.{}\".format(img_no, 'P-{:03d}'.format(current_page_no), current_img_url[current_img_url.rfind('/')+1:])\n","\n","            # Check if given response is actually an image\n","            if 'image' not in res.headers.get(\"content-type\", ''):\n","              \n","              # Not an image's response, so adding to error list\n","              current_r[1].append([VG[1],VG[2],current_img_url])\n","              print(current_img_url,'Image NOt found for ',VG_img_name,' ',VG[1])\n","            else:\n","\n","              # Good image, so getting it's binary content\n","              i = Image.open(io.BytesIO(res.content))\n","              # Saving image to file\n","              i.save(VG_img_name)\n","        \n","        # Increamenting page counter\n","        current_page_no += 1\n","\n","        # Check if last page reached\n","        if no_of_pages < current_page_no:\n","          # Getting next page url\n","          next_page_url = domain_name + next_pages[current_page_no - 1]['href']\n","\n","          # Assigning current page URL\n","          current_page_url = next_page_url\n","          \n","          # Requesting next page\n","          Scr_response = Scraper.get(next_page_url,headers=Headers)\n","\n","          # Check if status code is 200\n","          if Scr_response.status_code != 200:\n","            # Error requesting page, so add it to error list\n","            current_r[0].append([VG[1],current_page_url])\n","            # Terminating current Video game scraping\n","            return False\n","          else:\n","            # Parsing current response\n","            Scr_bs = bs(Scr_response.content, 'html.parser')\n","\n","    # Increamenting page, image counter\n","    current_r[4] += img_no\n","    current_r[5] += current_page_no\n","    current_r[6] += 1\n","\n","    # Calculating time took for each Video Game\n","    vg_scrape_time = time.time() - vg_scrape_time\n","\n","    # Creating a report for Scraping VG\n","    detail_content_raw = [\n","                          'Title    : {}'.format(VG[1]),\n","                          'URL      : {}'.format(VG[2]),\n","                          'Post ID  : {}'.format(VG[0]),\n","                          'No IMGs  : {}'.format(img_no),\n","                          'No Pages : {}'.format(current_page_no),\n","                          'Time Took: {:.2f} Seconds'.format(vg_scrape_time),\n","                          '\\n',\n","                          'IMG URLS :\\n{}'.format('\\n'.join(img_url_list))\n","                          \n","                          ]\n","    # Convert report from list to string\n","    detail_content = '\\n'.join(detail_content_raw)\n","\n","    # Writing report to file\n","    with open(VG_folder + 'Details.txt','w') as f1:\n","      f1.write(detail_content)\n","    \n","    # Updating status of Scraping\n","    status(current_r[6], VG[0], img_no, current_page_no, '{:.2f}'.format(vg_scrape_time), VG[1], VG[2])\n","  \n","  # If given Video game is already downloaded\n","  else:\n","    # Increament have count and add to list\n","    have[0] += 1\n","    have[1].append(VG[2])\n","    print('Have Count = ',have[0])\n","    \n","    # Updating progressbar by 1\n","    tqdm_bar.update(1)\n"," \n","# Function to update status of scraping\n","def status(a, b, c, d, e, f, g):\n","  print('{:03d} - ID:{:3s} - IMGs:{:03d} - Dirs:{:03d} - {:6s}Sec - {} - {}'.format(a, b, c, d, e, f, g))\n","\n","  # Updating progressbar by 1\n","  tqdm_bar.update(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HDvnGERCqPS","colab_type":"text"},"source":["#### Retry this 2 cells if connection error occured"]},{"cell_type":"code","metadata":{"id":"gHyJn40NBKXb","colab_type":"code","colab":{}},"source":["# Creating number of threads \n","pool1 = Pool(no_of_threads)\n","\n","# Creating progressbar wiht \"tqdm(full form 'taqaddum' means 'progress' in Arabic)\"\n","tqdm_bar = tqdm(total=len(all_VG_urls_and_title))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc2r5llHBVz7","colab_type":"code","colab":{}},"source":["\n","# Initializing Already downloaded list\n","have = [\n","           0, # Already have count\n","          [], # Already have post\n","        ]\n","\n","# Recording Scraping time\n","Scraping_imgs_time = time.time()\n","\n","# Running created threads\n","_ = pool1.map(VG_Scrape,all_VG_urls_and_title)\n","# Closing threads\n","pool1.close()\n","pool1.join()\n","\n","# Closing progress bar\n","tqdm_bar.close()\n","\n","# Calculating Scraping time\n","Scraping_imgs_time = time.time() - Scraping_imgs_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huy6zqoKBQgV","colab_type":"code","colab":{}},"source":["\n","# Report of Scraping\n","detail_content_raw = [\n","                          'Scraping took     : {:.2f} Seconds ({})'.format(Scraping_imgs_time, time.strftime('%H-%M-%S', time.gmtime(Scraping_imgs_time))),\n","                          'Total IMGs        : {}'.format(current_r[4]),\n","                          'Total Pages       : {}'.format(current_r[5]),\n","                          'Total Posts       : {}'.format(current_r[6]),\n","                          'Empty Posts {:03d}  : {}'.format(len(current_r[2]),current_r[2]),\n","                          'Error Posts {:03d}  : {}'.format(len(current_r[0]),current_r[0]),\n","                          'Error IMG   {:03d}  : {}'.format(len(current_r[1]),current_r[1]),\n","                          'Have Posts  {:03d}  : {}'.format(len(have[1]),have[1]),\n","                      ]\n","detail_content = '\\n'.join(detail_content_raw)\n","print(detail_content)\n","\n","# Writing Report to file\n","with open(Data_folder_Today+'Project-Details.txt','w') as f1:\n","  f1.write(detail_content)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F16ZZjGFYpVR","colab_type":"text"},"source":["##Directory indexing using JSON\n"]},{"cell_type":"code","metadata":{"id":"ELk0N0lJ-bn5","colab_type":"code","colab":{}},"source":["# Function to return file size in human understandable format\n","def file_size(size):\n","  power = 2**10\n","  n = 0\n","  power_labels = {0 : 'B', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB',5:'PB'}\n","  while size > power:\n","    size /= power\n","    n += 1\n","  return '{:.2f} {}'.format(size, power_labels[n])\n","\n","\n","# Function to return a directory index in dictionary format\n","def dirs_to_dict(i='', previous=''):\n","  \n","  # Creating current directory path\n","  fp = previous + i + '/'\n","  \n","  # Creating a sorted list of \"Directories\" in current directories\n","  sorted_dir = sorted([k for k in next(os.walk(fp))[1]])\n","  \n","  # Assigning current directory name and path\n","  temp = {\n","            keys_list[0] : i,   # Current Directory Name\n","            keys_list[1] : fp,  # Current Directory Path\n","          }\n","  \n","  # Getting a list of files with full path in current directory\n","  temp[keys_list[7]] =  [fp+j for j in sorted([k for k in next(os.walk(fp))[2]]) ]  \n","\n","  # Counting number of files in current directory\n","  temp[keys_list[2]] = len(temp[keys_list[7]])\n","  \n","  # Calculating total size of files in current directory\n","  temp[keys_list[3]] = sum([os.path.getsize(j) for j in temp[keys_list[7]]])\n","\n","  # Creating an empty list to store subdirectory's data\n","  temp[keys_list[8]] = []\n","  \n","  # Copying number files in current directory to initialize the calculation of total number of files inside current directory (including files in subdirectory)\n","  temp[keys_list[4]] = temp[keys_list[2]]\n","  \n","  # Inititalizing total directorys count to zero\n","  temp[keys_list[5]] = 0\n","  \n","  # Copying size of files in current directory to initialize the calculation of total size of files inside current directory (including files in subdirectory)\n","  temp[keys_list[6]] = temp[keys_list[3]]\n","  \n","  # Check if leaf directory is reached or not\n","  # If FALSE skip this part\n","  # If TRUE go inside \n","  if sorted_dir != []:\n","    \n","    # Traversing through each directory\n","    for j in sorted_dir:\n","    \n","      # Calling recursive function to it's subdirectorys and gets it's directory index\n","      temp_dict = dirs_to_dict(j, fp)\n","      \n","      # Getting files, directorys, file's size\n","      temp_Files_count, temp_Folders_count, temp_Size = temp_dict[keys_list[4]], temp_dict[keys_list[5]], temp_dict[keys_list[6]]\n","      \n","      # Appending subdirectory index to current directory's directory section\n","      temp[keys_list[8]].append(temp_dict)\n","      \n","      # Increamenting total files count inside current directory\n","      temp[keys_list[4]]  += temp_Files_count\n","      \n","      # Increamenting total directorys count inside current directory\n","      temp[keys_list[5]]  += temp_Folders_count + 1\n","      \n","      # Increamenting total file's size inside current directory\n","      temp[keys_list[6]]  += temp_Size\n","  if '[' not in fp:\n","    rename_list.append([fp, '{} [ {} - {} ]'.format(fp[:-1], temp[keys_list[4]], file_size(temp[keys_list[6]]))] )\n","\n","  # Adding a key that stores file size in readable format\n","  temp[keys_list[9]]  = file_size(temp[keys_list[6]])\n","\n","  # Returning index dictionary of current directory\n","  return temp\n","\n","# Assigning keys for dictionary\n","use_short_keys = 0\n","\n","# IF TRUE use shorter keys, else use longer meaningful keys\n","if use_short_keys == 1:\n","  keys_list = 'FoN#P#_CFi#_CS #_TFi#_TFo#_TS #__Fi #__Fo #_TSR#4'.split('#')\n","else:\n","  keys_list = 'Folder_Name   #Path          #_Current_Files#_Current_Size #_Total_Files  #_Total_Folders#_Total_Size   #__Files       #__Folders     #_TS_Readable  #14'.split('#')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZi_LmD-ouk7","colab_type":"code","colab":{}},"source":["# List to rename folders\n","# 1st value in each item points to \"Original Folder Name\"\n","# 2nd value in each item points to \"Renamed  Folder Name\"\n","rename_list = []\n","\n","# Directory name to index\n","dir_name = Root_folder_Today[:-1]\n","\n","# Calling indexing function and get the directory index\n","dir_data = dirs_to_dict(dir_name, '')\n","\n","print('JSON data Created')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDpAhM7gsjOI","colab_type":"text"},"source":["##Renaming Folders\n","\n","  Ex:\n","\n","    Original Folder name : God of War\n","\n","    Renamed  Folder name : God of War [ 35 - 3.67 MB]"]},{"cell_type":"code","metadata":{"id":"o-PDqm2qGScg","colab_type":"code","colab":{}},"source":["# Exclude from rename list\n","excluded_rename_list = [Root_folder_Today, Root_folder_Today[:-1]+'/.ipynb_checkpoints/']\n","rename_list = [i for i in rename_list if i[0] not in excluded_rename_list]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tnU_XzOE3qY","colab_type":"code","colab":{}},"source":["# Displaying first 10 items\n","rename_list[:11]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rV26T5ihpmfe","colab_type":"code","colab":{}},"source":["# Function to rename folders\n","def rename_folders(option):\n","  j, k = 1-option, 1*option\n","  for i in rename_list:\n","    os.rename(i[j], i[k])\n","\n","# If set 1 Rename folders\n","# If set 0 Undo Rename Folders\n","option = 1\n","rename_folders(option)\n","print('Folders renamed successfully')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPJhk7hZDNdP","colab_type":"text"},"source":["##Writing JSON file"]},{"cell_type":"code","metadata":{"id":"Wr968wJkC7u-","colab_type":"code","colab":{}},"source":["rename_list = []\n","\n","# Calling indexing function and get the directory index\n","dir_data = dirs_to_dict(dir_name, '')\n","\n","# Showing information of current directory from dictionary\n","for i in dir_data:\n","  if i not in [keys_list[7], keys_list[8]]:\n","    temp = '{:' + keys_list[10] + 's} -> {}'\n","    if i != keys_list[6]:\n","      print(temp.format(i, dir_data[i]))\n","    else:\n","      print(temp.format(i, file_size(dir_data[i])))\n","\n","# Assigning JSON filename\n","json_file_name = '{}/{} - Index.json'.format(dir_name, dir_name)\n","\n","# Writing dictionary in JSON format with \"Sorted Keys\" and \"UNICODE Support\" and \"Indentation\"\n","with open(json_file_name, 'w') as f1:\n","  json.dump(dir_data, f1, sort_keys=True, ensure_ascii=False, indent=4)\n","\n","print('Directory Index created successfully')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTyBBGpeozN9","colab_type":"text"},"source":["#Archiving Data"]},{"cell_type":"code","metadata":{"id":"nGptR_FJsrBn","colab_type":"code","colab":{}},"source":["\n","# Recording Archiving time\n","archive_time = time.time()\n","\n","# Archiving useing 7ZIP with no compression\n","!7z a -m0=Copy \"output_archieve_name\" \"$Root_folder_Today\"\n","\n","# Calculating Archiving time\n","archive_time = time.time() - archive_time\n","\n","print('\\n\\nArchieving took :\\n{:.2f} Seconds\\n{}'.format(archive_time, time.strftime('%H-%M-%S', time.gmtime(archive_time))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b8YvEgoDOBf","colab_type":"code","colab":{}},"source":["\n","# Calculating full program execution time\n","execution_time_full = time.time() - execution_time_full\n","\n","print('Full program took :\\n{:.2f} Seconds\\n{}'.format(execution_time_full, time.strftime('%HH-%MM-%SS', time.gmtime(execution_time_full))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmZt0oAGVhIs","colab_type":"code","colab":{}},"source":["# Creating archieve name\n","current_time = dati.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d_%H-%M-%S')\n","output_archieve_name = '{} - CreativeUncut ({}) ({}) ({}).7z'.format(current_time, current_r[6], current_r[4], time.strftime('%HH-%MM-%SS', time.gmtime(execution_time_full)))\n","\n","# Renaming Archived file\n","os.rename('output_archieve_name.7z', output_archieve_name)\n","print(output_archieve_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WV6RYjeEo59K","colab_type":"text"},"source":["#Copying Data"]},{"cell_type":"code","metadata":{"id":"2_3nkE4aWYX3","colab_type":"code","colab":{}},"source":["\n","# Recording time required to copy archive to Google drive\n","copy_time = time.time()\n","\n","# Copy Archive using linux built in copy function\n","!rsync -ah --progress \"$output_archieve_name\" \"/content/drive/My Drive/\"\n","\n","# Calculating copying time\n","copy_time = time.time() - copy_time\n","print('Copying took :\\n{:.2f} Seconds\\n{}'.format(copy_time, time.strftime('%H-%M-%S', time.gmtime(copy_time))))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVGZTzeUuMac","colab_type":"text"},"source":["# Other Test Features"]},{"cell_type":"markdown","metadata":{"id":"fZaqKQdIW0ZZ","colab_type":"text"},"source":["##Program to Download single image"]},{"cell_type":"code","metadata":{"id":"JNGK5CFkViql","colab_type":"code","colab":{}},"source":["#####################Program to Download Single image from creativeuncut.com#########################\n","\n","# import requests as rq\n","# import io\n","# from PIL import Image\n","\n","# Headers = {\n","#             'User-agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36',\n","#             'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n","#             'Accept-Encoding' : 'gzip,deflate,sdch',\n","#             'Referer' : 'https://www.creativeuncut.com/gallery-39/got-heavy-mongol-rank-4.html'\n","#           }\n","# llink = 'https://www.creativeuncut.com/gallery-39/art/got-heavy-mongol-rank-4.jpg'\n","\n","# rres = rq.get(llink, headers=Headers)\n","# i = Image.open(io.BytesIO(rres.content))\n","# i.save('1.jpg')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0QExYiFObR6","colab_type":"text"},"source":["Sort by file sizes"]},{"cell_type":"code","metadata":{"id":"Bu6sj6lC742u","colab_type":"code","colab":{}},"source":["##########################List all files with respective file size#########################\n","\n","# all_files_with_size = []\n","# total_size = 0\n","# for path, dirs, files in os.walk(start_path):\n","#   for f in files:\n","#     fp = os.path.join(path, f)\n","#     total_size += os.path.getsize(fp)\n","#     all_files_with_size.append([os.path.getsize(fp),f,fp])\n","# all_files_with_size = sorted(all_files_with_size, key=itemgetter(0), reverse=True)\n","\n","# n = 10\n","# print('First {} :\\n{}'.format(n, '\\n'.join(['{:10s}{}'.format(file_size(i[0]), i[1]) for i in all_files_with_size[:n+1]])))\n","# print('Last {} :\\n{}'.format(n, '\\n'.join(['{:10s}{}'.format(file_size(i[0]), i[1]) for i in all_files_with_size[-n:]])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHN1lIL2nid1","colab_type":"code","colab":{}},"source":["########################Function to get total files and total size of given folder ##########################\n","\n","# def get_size(start_path = '.'):\n","#   total_size = 0\n","#   total_count = 0\n","#   for path, dir, files in os.walk(start_path):\n","#     for f in files:\n","#       total_count += 1\n","#       total_size += os.path.getsize(os.path.join(path, f))\n","#   print('Total Files Size  = {}\\nTotal Files Count = {}'.format(file_size(total_size), total_count))\n","\n","# get_size(Data_folder_Today)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2TSZyQPMOhJ3","colab_type":"text"},"source":["Copy Speed comparison"]},{"cell_type":"code","metadata":{"id":"-yCToXMdFPa4","colab_type":"code","colab":{}},"source":["############################Speed comparison of !cp vs !rsync##################################\n","\n","\n","# !rm \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n","\n","# test2 = time.time()\n","# !cp \"/content/drive/My Drive/My Files/Project_Data/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\" \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n","# test2 = time.time() - test2\n","\n","# !rm \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n","\n","# test1 = time.time()\n","# !rsync -ah --progress \"/content/drive/My Drive/My Files/Project_Data/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\" \"/content/2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\"\n","# test1 = time.time() - test1\n","\n","# print('rsync took {:.2f} Sec\\ncp    took {:.2f} Sec'.format(test1, test2))\n","\n","\n","##########################Test Output###################################\n","# sending incremental file list\n","# 2020-09-11_15-46-29 - CreativeUncut (949) (52401).7z\n","#           5.15G 100%   57.60MB/s    0:01:25 (xfr#1, to-chk=0/1)\n","# rsync took 85.52 Sec\n","# cp    took 77.65 Sec\n","\n","\n","###########################Test result:################################\n","#  !cp is faster than !rsync with time gap of approximately 5 seconds\n","#  it is not much gap and !rsync shows progress\n","#\n","#  Between 5sec faster vs visual progress I will go for Visual progress so i am using rsync from now on\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CUij6HZF7fZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}
